import torch
import torch.nn as nn
from torchvision.datasets import MNIST
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from tqdm import tqdm

device = "mps" if torch.backends.mps.is_available() else "cpu"

class Encoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.l1 = nn.Sequential(
            nn.Linear(28*28, 256),
            nn.ReLU(),
            nn.Linear(256, 64),
            nn.ReLU(),
            nn.Linear(64, 16),
        )

    def forward(self, x):
        return self.l1(x)

class ConvEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),
            nn.ReLU(True),
            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),
            nn.ReLU(True),
            nn.Conv2d(32, 64, kernel_size=7)
        )
    
    def forward(self, x):
        x = self.encoder(x)
        return x


class Decoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.l1 = nn.Sequential(
            nn.Linear(16, 64),
            nn.ReLU(),
            nn.Linear(64, 256),
            nn.ReLU(),
            nn.Linear(256, 28*28),
        )

    def forward(self, x):
        return self.l1(x)


class ConvDecoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(64, 32, kernel_size=7),
            nn.ReLU(True),
            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.ReLU(True),
            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.Sigmoid() 
        )
    
    def forward(self, x):
        x = self.decoder(x)
        return x 



class DAE(nn.Module):
    def __init__(self, encoder, decoder):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder

    def forward(self, x):
        return self.decoder(self.encoder(x))

transform = transforms.ToTensor()
train_set = MNIST("/Users/aaronkang/Deep Learning/Datasets", download=False, train=True, transform=transform)
test_set = MNIST("/Users/aaronkang/Deep Learning/Datasets", download=False, train=False, transform=transform)

train_loader = DataLoader(train_set, batch_size=64, shuffle=True)
test_loader = DataLoader(test_set, batch_size=64, shuffle=True)

# model = DAE(Encoder(), Decoder())
# torch.compile(model)
criterion = nn.MSELoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)

def add_noise(img, noise_factor=0.5):
    noisy = img + noise_factor * torch.randn_like(img)
    noisy = torch.clamp(noisy, 0., 1.)
    mask = torch.rand_like(img)
    mask = (mask > noise_factor).float()
    noisy = noisy * mask
    return noisy



#Linear DAE Training Loop

# model = DAE(Encoder(), Decoder())
# torch.compile(model)
# criterion = nn.MSELoss()
# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)

# n_epochs = 25
# for epoch in range(n_epochs):
#     eloss = 0.0
#     for batch_idx, batch in enumerate(tqdm(train_loader)):
#         imgs, _ = batch
#         imgs = imgs.view(imgs.size(0), -1)
#         noisy_imgs = add_noise(imgs)
#         preds = model(noisy_imgs)
#         # preds = model(imgs)
#         loss = criterion(preds, imgs)
#         optimizer.zero_grad()
#         loss.backward()
#         optimizer.step()
#         eloss += loss.item()
#     print(f'Epoch: {epoch+1} | Loss: {eloss/len(train_loader)*100}')


#Convolutional DAE Training Loop
model = DAE(ConvEncoder(), ConvDecoder())
model.to(device)
torch.compile(model)
criterion = nn.MSELoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)

n_epochs = 25
for epoch in range(n_epochs):
    eloss = 0.0
    for batch_idx, (imgs, _) in enumerate(tqdm(train_loader)):
        noisy_imgs = add_noise(imgs)
        imgs, noisy_imgs = imgs.to(device), noisy_imgs.to(device)
        preds = model(noisy_imgs)
        loss = criterion(preds, imgs)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        eloss += loss.item()
    print(f'Epoch: {epoch+1} | Loss: {eloss/len(train_loader)*100}')
